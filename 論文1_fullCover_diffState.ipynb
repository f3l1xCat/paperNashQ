{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27fdbdb-e86d-431e-a454-e75b8fba0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "# pip install nashpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7be27e-6667-430d-9ad7-031e696d90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[30]:\n",
    "\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29973b90-650b-4a28-ae57-16ee9b46e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[31]:\n",
    "\n",
    "import numpy as np\n",
    "import nashpy as ns\n",
    "from itertools import permutations\n",
    "import lemkeHowson1 as lemkeHowson\n",
    "import matrix\n",
    "import rational\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4502c7b-7bf6-436a-9212-69b43b012103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[32]:\n",
    "\n",
    "u = 1\n",
    "t = 0.5\n",
    "beta = 0.5\n",
    "q = 1.3\n",
    "lambda_ = 0.2\n",
    "delta = 0.3\n",
    "big_A = 2\n",
    "eta = 0 # state location\n",
    "WORLD_HEIGHT = 30\n",
    "WORLD_WIDTH = 30\n",
    "\n",
    "# possible actions\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_STOP = 2\n",
    "\n",
    "# 建座標平面\n",
    "gridIndexList = []\n",
    "for i in range(0, WORLD_HEIGHT+1):\n",
    "    gridIndexList.append(i)\n",
    "\n",
    "actions = [ACTION_UP, ACTION_DOWN]\n",
    "statesAllOne = []\n",
    "locationValidActions = {}\n",
    "history_state_agent0 = []\n",
    "history_state_agent1 = []\n",
    "resagentStateList={}\n",
    "resState={}\n",
    "\n",
    "# 建立state的所有可能(有兩個agent)\n",
    "for i in permutations(gridIndexList, 2):\n",
    "    statesAllOne.append(i)\n",
    "\n",
    "for i in range(WORLD_HEIGHT+1):\n",
    "    statesAllOne.append((i, i))\n",
    "\n",
    "for i in gridIndexList:\n",
    "    locationValidActions[i] = []\n",
    "\n",
    "# 每個位置皆有自己的action可能\n",
    "for i in range(0, WORLD_HEIGHT+1):\n",
    "    gridIndexNumber = i\n",
    "    if i != WORLD_HEIGHT:\n",
    "        locationValidActions[gridIndexNumber].append(ACTION_UP)\n",
    "    if i != 0:\n",
    "        locationValidActions[gridIndexNumber].append(ACTION_DOWN)\n",
    "    locationValidActions[gridIndexNumber].append(ACTION_STOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77510b3-29f4-41de-bb0e-c653b3e701e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[33]:\n",
    "\n",
    "class agent:\n",
    "    def __init__(self, agentIndex = 0, startLocationIndex = eta):\n",
    "        self.epsilon = 1.0\n",
    "        self.goalState = ()\n",
    "        self.qTable = {}\n",
    "        self.timeNumber = {}\n",
    "        self.alpha = {}\n",
    "        # self.singleQValue = {}\n",
    "        # self.singleAlpha = {}\n",
    "        self.currentState = ()\n",
    "        self.nextState = ()\n",
    "        self.strategy = {}\n",
    "        self.agentIndex = agentIndex\n",
    "        self.startLocationIndex = startLocationIndex\n",
    "        self.locationIndex = startLocationIndex\n",
    "        self.currentAction = 0\n",
    "        self.currentReward = 0\n",
    "        self.timeStep = 0\n",
    "\n",
    "\n",
    "    # This is not necessary in this experiment\n",
    "    def initialSelfStrategy(self):\n",
    "        for i in statesAllOne:\n",
    "            self.strategy[i] = {}\n",
    "            for j in locationValidActions[i[self.agentIndex]]:\n",
    "                #i here is the combination of two agents' location index\n",
    "                #agentIndex represents the agent number\n",
    "                #initial the strategy, the probability of all action is 0\n",
    "                self.strategy[i][j] = 0\n",
    "\n",
    "    def initialSelfQTable(self):\n",
    "        # agent0 and agnet1\n",
    "        # each agent keeps two tables: one for himself and for the opponent\n",
    "        # in Qtable, agent also can observe his opponent's action\n",
    "        self.qTable[0] = {}\n",
    "        self.qTable[1] = {}\n",
    "        for i in statesAllOne:\n",
    "            self.qTable[0][i] = {}\n",
    "            self.qTable[1][i] = {}\n",
    "            for j_1 in locationValidActions[i[0]]:\n",
    "                for j_2 in locationValidActions[i[1]]:\n",
    "                    self.qTable[0][i][(j_1, j_2)] = 0\n",
    "                    self.qTable[1][i][(j_1, j_2)] = 0\n",
    "\n",
    "    def initialSelfAlpha(self):\n",
    "        # account the visiting number of each combination of states and actions\n",
    "        for i in statesAllOne:\n",
    "            self.alpha[i] = {}\n",
    "            self.timeNumber[i] = {}\n",
    "            for j_1 in locationValidActions[i[0]]:\n",
    "                for j_2 in locationValidActions[i[1]]:\n",
    "                    self.alpha[i][(j_1, j_2)] = 0\n",
    "                    self.timeNumber[i][(j_1, j_2)] = 0\n",
    "                    \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = self.epsilon * 0.999\n",
    "        if self.epsilon < 0.01:\n",
    "            self.epsilon = 0.01\n",
    "\n",
    "    # def initialSingleQValue(self):\n",
    "    #     for i in statesAllOne:\n",
    "    #         self.singleQValue[i] = {}\n",
    "    #         for j in locationValidActions[i[self.agentIndex]]:\n",
    "    #             self.singleQValue[i][j] = 0\n",
    "    #\n",
    "    # def initialSingleAlpha(self):\n",
    "    #     for i in statesAllOne:\n",
    "    #         self.singleAlpha[i] = {}\n",
    "    #         for j in locationValidActions[i[self.agentIndex]]:\n",
    "    #             self.singleAlpha[i][j] = 0\n",
    "\n",
    "    # def chooseActionWithEpsilon(self, EPSILON, currentState):\n",
    "    #     self.locationIndex = currentState[self.agentIndex]\n",
    "    #     if np.random.binomial(1, self.EPSILON) == 1:\n",
    "    #         self.currentAction = np.random.choice(locationValidActions[self.locationIndex])\n",
    "    #     else:\n",
    "    #         # it is a method to find the max value in a dict and the corresponding key\n",
    "    #         self.currentAction = max(zip(self.singleQValue[self.currentState].values(), self.singleQValue[self.currentState].keys()))[1]\n",
    "\n",
    "\n",
    "    def constructPayoffTable(self, state):\n",
    "        # construct Payoff Table for agent 0 and agent 1\n",
    "        # actions0 and actions1 are list for invalid actions\n",
    "        # the content of Payoff Table is the Q value\n",
    "        # 每個state的各個agent的各自action轉換成矩陣\n",
    "        actions0 = locationValidActions[state[0]]\n",
    "        actions1 = locationValidActions[state[1]]\n",
    "        m0 = matrix.Matrix(len(actions0), len(actions1))\n",
    "        m1 = matrix.Matrix(len(actions0), len(actions1))\n",
    "        for i in range(len(actions0)):\n",
    "            for j in range(len(actions1)):\n",
    "                m0.setItem(i+1, j+1, self.qTable[0][state][(actions0[i], actions1[j])])\n",
    "                m1.setItem(i+1, j+1, self.qTable[1][state][(actions0[i], actions1[j])])\n",
    "        return (m0, m1)\n",
    "\n",
    "    def nashQLearning(self, gamma, agent0Action, agent0Reward, currentState, nextState, agent1Action, agent1Reward):\n",
    "        self.gamma = gamma\n",
    "        self.currentState = currentState\n",
    "        self.nextState = nextState\n",
    "        # 這個state中每個agent做action的次數\n",
    "        self.timeNumber[self.currentState][(agent0Action, agent1Action)] += 1\n",
    "        self.alpha[self.currentState][(agent0Action, agent1Action)] = 1.0 / self.timeNumber[self.currentState][(agent0Action, agent1Action)]\n",
    "        # m0 and m1 are payoff tables of agent0 and agent1 (based on Qvalue table)\n",
    "#         print('state: ', currentState, 'agent0Action: ', agent0Action, 'agent1Action: ', agent1Action)\n",
    "        (m0, m1) = self.constructPayoffTable(nextState)\n",
    "#         print('m0, m1', m0, m1)\n",
    "        probprob = lemkeHowson.lemkeHowson(m0, m1)\n",
    "#         print(\"probprob\", probprob)\n",
    "        prob0 = np.array(probprob[0])\n",
    "        prob1 = np.array(probprob[1])\n",
    "        prob0 = np.matrix(prob0)\n",
    "        prob1 = np.matrix(prob1).reshape((-1, 1))\n",
    "        # calculate the nash values\n",
    "        m_m0 = []\n",
    "        m_m1 = []\n",
    "        for i in range(m0.getNumRows()):\n",
    "            for j in range(m0.getNumCols()):\n",
    "                m_m0.append(m0.getItem(i+1, j+1))\n",
    "        for i in range(m1.getNumRows()):\n",
    "            for j in range(m1.getNumCols()):\n",
    "                m_m1.append(m1.getItem(i+1, j+1))\n",
    "        m_m0 = np.matrix(m_m0).reshape((m0.getNumRows(), m0.getNumCols()))\n",
    "        m_m1 = np.matrix(m_m1).reshape((m1.getNumRows(), m1.getNumCols()))\n",
    "        m_nash0 = prob0 * m_m0 * prob1\n",
    "        m_nash1 = prob0 * m_m1 * prob1\n",
    "        nash0 = m_nash0[0, 0].nom() / m_nash0[0, 0].denom()\n",
    "        nash1 = m_nash1[0, 0].nom() / m_nash1[0, 0].denom()\n",
    "        nashQValues = [nash0, nash1]\n",
    "        self.qTable[0][self.currentState][(agent0Action, agent1Action)]             = (1 - self.alpha[self.currentState][(agent0Action, agent1Action)])                 * self.qTable[0][self.currentState][(agent0Action, agent1Action)]                     + self.alpha[self.currentState][(agent0Action, agent1Action)]                         * (agent0Reward + self.gamma * nashQValues[0])\n",
    "        self.qTable[1][self.currentState][(agent0Action, agent1Action)]             = (1 - self.alpha[self.currentState][(agent0Action, agent1Action)])                 * self.qTable[1][self.currentState][(agent0Action, agent1Action)]                     + self.alpha[self.currentState][(agent0Action, agent1Action)]                         * (agent1Reward + self.gamma * nashQValues[1])\n",
    "#         print(self.qTable[0][self.currentState][(agent0Action, agent1Action)] == self.qTable[1][self.currentState][(agent0Action, agent1Action)])\n",
    "        self.timeStep += 1\n",
    "\n",
    "    def chooseActionBasedOnQTable(self, currentState):\n",
    "        self.locationIndex = currentState[self.agentIndex]\n",
    "        (m0, m1) = self.constructPayoffTable(currentState)\n",
    "        probprob = lemkeHowson.lemkeHowson(m0, m1)\n",
    "        prob0 = np.array(probprob[0])\n",
    "        re0 = np.where(prob0 == np.max(prob0))[0][0]\n",
    "        prob1 = np.array(probprob[1])\n",
    "        re1 = np.where(prob1 == np.max(prob1))[0][0]\n",
    "        re = [re0, re1]\n",
    "        actionsAvailable = locationValidActions[currentState[self.agentIndex]]\n",
    "        return actionsAvailable[re[self.agentIndex]]\n",
    "    \n",
    "    def chooseActionRandomly(self, currentState):\n",
    "        # choose action randomly based on current location\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            self.locationIndex = currentState[self.agentIndex]\n",
    "            self.currentAction = np.random.choice(locationValidActions[self.locationIndex])\n",
    "#             self.update_epsilon()\n",
    "#             print(self.epsilon)\n",
    "        else:\n",
    "            self.currentAction = np.random.choice(locationValidActions[self.locationIndex])\n",
    "#             self.currentAction = self.chooseActionBasedOnQTable(currentState)\n",
    "            \n",
    "        return self.currentAction\n",
    "    \n",
    "    def chooseActionBasedOnQTableRandomly(self, currentState):\n",
    "        # choose action randomly based on current location\n",
    "        if np.random.uniform() < 0.05:\n",
    "            self.locationIndex = currentState[self.agentIndex]\n",
    "            self.currentAction = np.random.choice(locationValidActions[self.locationIndex])\n",
    "#             self.update_epsilon()\n",
    "#             print(self.epsilon)\n",
    "        else:\n",
    "#             self.currentAction = np.random.choice(locationValidActions[self.locationIndex])\n",
    "            self.currentAction = self.chooseActionBasedOnQTable(currentState)\n",
    "            \n",
    "        return self.currentAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443d6bbe-4fee-4101-aa07-6c443d606606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[34]:\n",
    "\n",
    "def nextGridIndex(action, gridIndex):\n",
    "    action = action\n",
    "    index_i = gridIndex\n",
    "    if (action == 0): # 0 = +1\n",
    "        index_i += 1\n",
    "    elif (action == 1): # 1 = -1\n",
    "        index_i -= 1\n",
    "    # 2 = 不動，所以不用判斷直接回傳\n",
    "    nextIndex = index_i\n",
    "    return nextIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da48f81c-967c-466f-8a2a-83fb381cd046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[35]:\n",
    "\n",
    "def gridGameOne(action_0, action_1, currentState, count, agent_0, agent_1):\n",
    "    action_0 = action_0\n",
    "    action_1 = action_1\n",
    "    agent_0 = agent_0\n",
    "    agent_1=agent_1\n",
    "    currentState = currentState\n",
    "    reward_0 = 0\n",
    "    reward_1 = 0\n",
    "    endGameFlag = 0\n",
    "\n",
    "    currentIndex_0 = currentState[0] # agent 0's current location\n",
    "    currentIndex_1 = currentState[1] # agent 1's current location\n",
    "    nextIndex_0 = nextGridIndex(action_0, currentState[0]) # agent 0's next location\n",
    "    nextIndex_1 = nextGridIndex(action_1, currentState[1]) # agent 1's next location\n",
    "\n",
    "    # construct payoff table for agent 0 and agent 1 and test if there's nash equilibrium\n",
    "    (m0, m1) = agent_0.constructPayoffTable((nextIndex_0, nextIndex_1))\n",
    "    testlemkeHowson_agent0 = lemkeHowson.testlemkeHowson(m0, m1)\n",
    "    (m0, m1) = agent_1.constructPayoffTable((nextIndex_0, nextIndex_1))\n",
    "    testlemkeHowson_agent1 = lemkeHowson.testlemkeHowson(m0, m1)\n",
    "\n",
    "    alpha = round(nextIndex_0/10,1) \n",
    "    p = round(nextIndex_1/10,1)\n",
    "   \n",
    "    theta_bar = (u - beta * alpha) / t\n",
    "    theta_hat = (t - lambda_ + (1 - q) * u - beta * alpha + p) / (2*t - lambda_)\n",
    "    theta_uline = (2 * t * t + (-2 * q * u + 2 * p - 2 * lambda_)*t + lambda_ * (u - beta * alpha)) / (t * (2 * t - lambda_))\n",
    "    if (theta_bar >= 0 and theta_bar <= 1):\n",
    "        Ng = theta_hat + delta * (theta_bar - theta_hat)\n",
    "    else:\n",
    "        Ng = theta_hat + delta * (1 - theta_hat)\n",
    "    \n",
    "    Nc = 1 - theta_hat\n",
    "    U_google = u - t * theta_hat - beta * alpha\n",
    "    U_chatgpt = q * u + lambda_ * Nc - t * (1 - theta_hat) - p\n",
    "    \n",
    "#     if testlemkeHowson_agent0==False or testlemkeHowson_agent1==False:\n",
    "#         print(currentState, (nextIndex_0, nextIndex_1), testlemkeHowson_agent0, testlemkeHowson_agent1)\n",
    "# 超過邊界或是效用為0都不成立\n",
    "    \n",
    "    if (nextIndex_0 < 0 or nextIndex_1 < 0) or (nextIndex_0 > WORLD_HEIGHT or nextIndex_1 > WORLD_WIDTH) or U_google < 0 or U_chatgpt < 0 or (theta_bar < theta_hat) or theta_hat < theta_uline or (theta_hat < 0 or theta_hat > 1):\n",
    "        reward_0 = -100\n",
    "        reward_1 = -100\n",
    "        if testlemkeHowson_agent0==False or testlemkeHowson_agent1==False:\n",
    "            reward_0=0\n",
    "            reward_1=0\n",
    "        nextState = (currentIndex_0, currentIndex_1)\n",
    "        \n",
    "# #       還要減去(x係數減y係數)\n",
    "#         if 2*(K-beta*sigma*phi)*currentIndex_0-K*currentIndex_1 >= delta*K+t*(K-beta*sigma)-(2*(K-beta*sigma*phi)-K) and \\\n",
    "#             (beta*sigma*phi-K)*currentIndex_0+2*K*currentIndex_1 >= delta*K+t*(beta*sigma-K)+K*beta*M-((beta*sigma*phi-K)+2*K):\n",
    "#             endGameFlag = 1\n",
    "#             reward_0 = 10\n",
    "#             reward_1 = 10\n",
    "        \n",
    "    else:\n",
    "#         theta = ((delta+t+nextIndex_1-nextIndex_0)*K+beta*sigma*phi*nextIndex_0-beta*sigma*t)/((2*delta+beta*M)*K)\n",
    "#         d_a=(phi*nextIndex_0-t)/K\n",
    "#         U_uber = V-delta*theta-beta*(theta*M-sigma*d_a)-nextIndex_0\n",
    "#         U_buget = V-delta*(1-theta)-t-nextIndex_1\n",
    "        r0 = alpha * (big_A - alpha) * Ng\n",
    "        r1 = p * Nc\n",
    "        # r1 = (1-phi)*nextIndex_0*theta*M\n",
    "        # if eta >= (1-theta)*M:\n",
    "            # r2 = nextIndex_1*(1-theta)*M-c*eta\n",
    "        # else:\n",
    "#             r2 = 0\n",
    "            # r2 = nextIndex_1*eta-c*eta\n",
    "        count += 1\n",
    "        reward_0 = r0\n",
    "        reward_1 = r1\n",
    "\n",
    "#         if r1 < 0 or r2 < 0:\n",
    "#             reward_0 = -10\n",
    "#             reward_1 = -10\n",
    "#         elif r1 > 0 and r2 < 0:\n",
    "#             reward_1 = -10\n",
    "#         elif r1 < 0 and r2 > 0:\n",
    "#             reward_0 = -10\n",
    "                \n",
    "        nextState = (nextIndex_0, nextIndex_1)\n",
    "    \n",
    "    if count > 2000:\n",
    "        endGameFlag = 1\n",
    "        \n",
    "    return reward_0, reward_1, nextState, count, endGameFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2581026b-dc92-470f-8ffb-3422bd98bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[36]:\n",
    "\n",
    "def resetStartState():\n",
    "#     print(history_state_agent0)\n",
    "#     newLocationIndex = np.random.choice(len(history_state_agent0))\n",
    "#     agent_0LocationIndex = history_state_agent0[newLocationIndex][0]\n",
    "#     agent_1LocationIndex = history_state_agent0[newLocationIndex][1]\n",
    "#     return (agent_0LocationIndex, agent_1LocationIndex)\n",
    "    return (eta, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82bfd4eb-a9c6-4df8-899a-f55dcecbdc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[37]:\n",
    "\n",
    "def playGameOne(agent_0 = agent, agent_1 = agent):\n",
    "    gamma = 0.9\n",
    "    agent_0 = agent_0\n",
    "    agent_1 = agent_1\n",
    "    count = 0\n",
    "    currentState = (agent_0.startLocationIndex, agent_1.startLocationIndex)\n",
    "    timeStep = 0 # calculate the timesteps in one episode\n",
    "    episodes = 0\n",
    "    endGameFlag = 0\n",
    "    agent_0.initialSelfQTable()\n",
    "    agent_1.initialSelfQTable()\n",
    "    agent_0.initialSelfAlpha()\n",
    "    agent_1.initialSelfAlpha()\n",
    "    while episodes < 100:\n",
    "        new = 0\n",
    "#         print (episodes)\n",
    "        while new == 0:\n",
    "            agent0Action = agent_0.chooseActionRandomly(currentState)\n",
    "            agent1Action = agent_1.chooseActionRandomly(currentState)\n",
    "            reward_0, reward_1, nextState, count, endGameFlag= gridGameOne(agent0Action, agent1Action, currentState, count, agent_0, agent_1)\n",
    "#             print(currentState, nextState)\n",
    "            agent_0.nashQLearning(gamma, agent0Action, reward_0, currentState, nextState, agent1Action, reward_1)\n",
    "            if currentState not in history_state_agent0:\n",
    "                history_state_agent0.append(currentState)\n",
    "            agent_1.nashQLearning(gamma, agent0Action, reward_0, currentState, nextState, agent1Action, reward_1)\n",
    "#             if currentState[0] > 13 and currentState[1] > 13:\n",
    "#                 print('currentState: ', currentState)\n",
    "            currentState = nextState\n",
    "            if (endGameFlag == 1): # one episode of the game is end\n",
    "                episodes += 1\n",
    "                currentState = resetStartState()\n",
    "                count = 0\n",
    "#                 agent_0.epsilon = 1.0\n",
    "#                 agent_1.epsilon = 1.0\n",
    "                new = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430d79d2-28e6-4603-8647-e3e628891d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[38]:\n",
    "\n",
    "runs = 0\n",
    "agentActionListEveryRun = {}\n",
    "\n",
    "# agent_0 = agent(agentIndex=0, startLocationIndex=0)\n",
    "# agent_1 = agent(agentIndex=1, startLocationIndex=2)\n",
    "# playGameOne(agent_0, agent_1)\n",
    "# agentActionListEveryRun[runs] = test(agent_0, agent_1)\n",
    "# print (agentActionListEveryRun)\n",
    "\n",
    "# for runs in range(50):\n",
    "#     agent_0 = agent(agentIndex=0, startLocationIndex=0)\n",
    "#     agent_1 = agent(agentIndex=1, startLocationIndex=2)\n",
    "#     playGameOne(agent_0, agent_1)\n",
    "#     agentActionListEveryRun[runs] = test(agent_0, agent_1)\n",
    "#     print (runs)\n",
    "# nashnum = 0\n",
    "# for runs in range(50):\n",
    "#     if agentActionListEveryRun[runs][4] == (8, 6):\n",
    "#         nashnum += 1\n",
    "# print (agentActionListEveryRun)\n",
    "# print (nashnum)\n",
    "\n",
    "# 需要判斷起點從哪一點開始\n",
    "# start=int((delta*K+t*(K-beta*sigma))/(2*(K-beta*sigma*phi)))\n",
    "agent_0 = agent(agentIndex=0, startLocationIndex=eta)\n",
    "agent_1 = agent(agentIndex=1, startLocationIndex=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96917c1a-17a9-4c9d-b280-6c89130ceb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[39]:\n",
    "\n",
    "def test (agent_0 = agent, agent_1 = agent):\n",
    "    agent_0 = agent_0\n",
    "    agent_1 = agent_1\n",
    "    startState = (eta, eta)\n",
    "    endGameFlag = 0\n",
    "    runs = 0\n",
    "    count = 0\n",
    "    agentActionList = []\n",
    "    agentStateList=[]\n",
    "    finalState=()\n",
    "    currentState = startState\n",
    "    endGameFlag = 0\n",
    "    while endGameFlag != 1:\n",
    "        agent0Action = agent_0.chooseActionBasedOnQTableRandomly(currentState)\n",
    "        agent1Action = agent_1.chooseActionBasedOnQTableRandomly(currentState)\n",
    "        agentActionList.append([agent0Action, agent1Action])\n",
    "        reward_0, reward_1, nextState, count, endGameFlag = gridGameOne(agent0Action, agent1Action, currentState, count, agent_0, agent_1)\n",
    "        agentStateList.append(currentState)\n",
    "        currentState = nextState\n",
    "#         print(currentState)\n",
    "    agentStateList.append(currentState)\n",
    "    finalState=currentState\n",
    "    print(currentState)\n",
    "    return agentActionList, agentStateList, finalState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb07e309-9151-400b-8b3d-6bea76904fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[40]:\n",
    "\n",
    "def rungame (agent_0 = agent, agent_1 = agent):\n",
    "    agent_0 = agent_0\n",
    "    agent_1 = agent_1\n",
    "    playGameOne(agent_0, agent_1)\n",
    "#     print('-----------------------------------------------------------')\n",
    "#     print('test: ')\n",
    "    runGameResult, agentStateList, finalState = test(agent_0, agent_1)\n",
    "#     print(runGameResult)\n",
    "    return runGameResult, agentStateList, finalState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a04ef950-ed1c-4cbe-bf23-47fcbcf317e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta:0\n",
      "0\n",
      "(5, 6)\n",
      "1\n",
      "(9, 8)\n",
      "2\n",
      "(6, 4)\n",
      "0 [(5, 6), (9, 8), (6, 4)]\n",
      "eta:1\n",
      "0\n",
      "(9, 8)\n",
      "1\n",
      "(6, 5)\n",
      "2\n",
      "(9, 9)\n",
      "1 [(9, 8), (6, 5), (9, 9)]\n",
      "eta:2\n",
      "0\n",
      "(10, 8)\n",
      "1\n",
      "(9, 8)\n",
      "2\n",
      "(6, 6)\n",
      "2 [(10, 8), (9, 8), (6, 6)]\n",
      "eta:3\n",
      "0\n",
      "(9, 7)\n",
      "1\n",
      "(9, 9)\n",
      "2\n",
      "(9, 7)\n",
      "3 [(9, 7), (9, 9), (9, 7)]\n",
      "eta:4\n",
      "0\n",
      "(11, 6)\n",
      "1\n",
      "(6, 5)\n",
      "2\n",
      "(6, 8)\n",
      "4 [(11, 6), (6, 5), (6, 8)]\n",
      "eta:5\n",
      "0\n",
      "(6, 6)\n",
      "1\n",
      "(9, 7)\n",
      "2\n",
      "(11, 6)\n",
      "5 [(6, 6), (9, 7), (11, 6)]\n",
      "eta:6\n",
      "0\n",
      "(10, 7)\n",
      "1\n",
      "(10, 8)\n",
      "2\n",
      "(11, 5)\n",
      "6 [(10, 7), (10, 8), (11, 5)]\n",
      "eta:7\n",
      "0\n",
      "(7, 6)\n",
      "1\n",
      "(8, 8)\n",
      "2\n",
      "(8, 6)\n",
      "7 [(7, 6), (8, 8), (8, 6)]\n",
      "eta:8\n",
      "0\n",
      "(9, 7)\n",
      "1\n",
      "(11, 7)\n",
      "2\n",
      "(7, 5)\n",
      "8 [(9, 7), (11, 7), (7, 5)]\n",
      "eta:9\n",
      "0\n",
      "(4, 6)\n",
      "1\n",
      "(7, 6)\n",
      "2\n",
      "(11, 8)\n",
      "9 [(4, 6), (7, 6), (11, 8)]\n",
      "eta:10\n",
      "0\n",
      "(8, 7)\n",
      "1\n",
      "(8, 6)\n",
      "2\n",
      "(7, 7)\n",
      "10 [(8, 7), (8, 6), (7, 7)]\n",
      "eta:11\n",
      "0\n",
      "(8, 8)\n",
      "1\n",
      "(8, 7)\n",
      "2\n",
      "(9, 7)\n",
      "11 [(8, 8), (8, 7), (9, 7)]\n",
      "eta:12\n",
      "0\n",
      "(7, 7)\n",
      "1\n",
      "(9, 8)\n",
      "2\n",
      "(9, 8)\n",
      "12 [(7, 7), (9, 8), (9, 8)]\n",
      "eta:13\n",
      "0\n",
      "(7, 5)\n",
      "1\n",
      "(6, 6)\n",
      "2\n",
      "(9, 8)\n",
      "13 [(7, 5), (6, 6), (9, 8)]\n",
      "eta:14\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 16\u001b[0m     action, state, finalState \u001b[38;5;241m=\u001b[39m \u001b[43mrungame\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     agentActionList\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m     18\u001b[0m     agentStateList\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m, in \u001b[0;36mrungame\u001b[1;34m(agent_0, agent_1)\u001b[0m\n\u001b[0;32m      4\u001b[0m     agent_0 \u001b[38;5;241m=\u001b[39m agent_0\n\u001b[0;32m      5\u001b[0m     agent_1 \u001b[38;5;241m=\u001b[39m agent_1\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mplayGameOne\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     print('-----------------------------------------------------------')\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     print('test: ')\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     runGameResult, agentStateList, finalState \u001b[38;5;241m=\u001b[39m test(agent_0, agent_1)\n",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m, in \u001b[0;36mplayGameOne\u001b[1;34m(agent_0, agent_1)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#         print (episodes)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m new \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m             agent0Action \u001b[38;5;241m=\u001b[39m \u001b[43magent_0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseActionRandomly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrentState\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m             agent1Action \u001b[38;5;241m=\u001b[39m agent_1\u001b[38;5;241m.\u001b[39mchooseActionRandomly(currentState)\n\u001b[0;32m     22\u001b[0m             reward_0, reward_1, nextState, count, endGameFlag\u001b[38;5;241m=\u001b[39m gridGameOne(agent0Action, agent1Action, currentState, count, agent_0, agent_1)\n",
      "Cell \u001b[1;32mIn[5], line 152\u001b[0m, in \u001b[0;36magent.chooseActionRandomly\u001b[1;34m(self, currentState)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[0;32m    151\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocationIndex \u001b[38;5;241m=\u001b[39m currentState[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magentIndex]\n\u001b[1;32m--> 152\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentAction \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocationValidActions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocationIndex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m#             self.update_epsilon()\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m#             print(self.epsilon)\u001b[39;00m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrentAction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(locationValidActions[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocationIndex])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# In[41]:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # playGameOne(agent_0, agent_1)\n",
    "    pool = multiprocessing.Pool(processes=10)\n",
    "    for i in range(0, 31):\n",
    "        eta = i\n",
    "        print(f\"eta:{eta}\")\n",
    "        agent_0 = agent(agentIndex=0, startLocationIndex=eta)\n",
    "        agent_1 = agent(agentIndex=1, startLocationIndex=eta)\n",
    "        agentActionList = []\n",
    "        agentStateList = []\n",
    "        finalStateList = []\n",
    "        for i in range(3):\n",
    "            print(i)\n",
    "            action, state, finalState = rungame(agent_0, agent_1)\n",
    "            agentActionList.append(action)\n",
    "            agentStateList.append(state)\n",
    "            finalStateList.append(finalState)\n",
    "        resagentStateList[eta]=agentStateList #每個eta的agentStateList(在test中每個episode的state)\n",
    "        print(eta, finalStateList)\n",
    "        resState[eta]=finalStateList # 每個eta的finalStateList(每個test最後一個停住的state)\n",
    "        \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    # print (agent_0.qTable[1][3, 6])\n",
    "    # print (agent_1.qTable[0][8, 5])\n",
    "    # print (agent_1.qTable[1][8, 5])\n",
    "#     for res in agentActionList:\n",
    "#         print(res)\n",
    "#         print (res.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0c203-1b43-4f56-9dbd-9394f6866d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# In[43]:\n",
    "\n",
    "k=eta\n",
    "for i in range(len(resState[k])): # 17-20 = 4\n",
    "    alpha = []\n",
    "    price = []\n",
    "    print(k)\n",
    "    eposide = [i for i in range(len(resagentStateList[k][i]))]\n",
    "    for j in range(len(resagentStateList[k][i])):\n",
    "        # print(i)\n",
    "        alpha.append(resagentStateList[k][i][j][0])\n",
    "        price.append(resagentStateList[k][i][j][1])\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(eposide, alpha, \"r\")\n",
    "    plt.plot(eposide, price, \"g\")\n",
    "    plt.legend(labels=['alpha', 'price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b432be-ea49-4028-8db9-07eafddeb676",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# In[44]:\n",
    "\n",
    "import csv\n",
    "resState_info=['eta', 'pricing']\n",
    "print(type(resState))\n",
    "with open('paper_diffstate', 'a+') as f:\n",
    "    writer=csv.writer(f)\n",
    "    for k, v in resState.items():\n",
    "        writer.writerow([k, v[0], v[1], v[2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nashqthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
